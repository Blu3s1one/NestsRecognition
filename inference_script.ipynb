{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb32a80-0e1b-4ddf-b2ff-1fe20611345b",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e496b5c5-b8f1-40b3-821c-ed62af8f54ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import ast\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import neptune\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "\n",
    "from scripts.datasets import ObjectDetectionDatasetSingle, ObjectDetectionDataSet\n",
    "from scripts.faster_RCNN import get_fasterRCNN_resnet\n",
    "from scripts.transformations import ComposeDouble\n",
    "from scripts.transformations import ComposeSingle\n",
    "from scripts.transformations import FunctionWrapperDouble\n",
    "from scripts.transformations import FunctionWrapperSingle\n",
    "from scripts.transformations import apply_nms, apply_score_threshold\n",
    "from scripts.transformations import normalize_01\n",
    "from scripts.utils import get_filenames_of_path, collate_single, save_json\n",
    "from scripts.visual import DatasetViewer\n",
    "from scripts.visual import DatasetViewerSingle\n",
    "from scripts.balayage import *\n",
    "from skimage.io import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88f582ef-cf8b-4ddd-bfce-e986bfa6a87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "params = {'INPUT_DIR': 'Input',  # files to predict\n",
    "          'PREDICTIONS_PATH': 'Output',  # where to save the predictions\n",
    "          'MODEL_DIR': 'model/checkpoint.ckpt',  # load model from checkpoint\n",
    "          }\n",
    "parameters = {'BATCH_SIZE': 2,\n",
    "          'OWNER': 'bluestone',  # set your name here, e.g. johndoe22\n",
    "          'SAVE_DIR': None,  # checkpoints will be saved to cwd\n",
    "          'GPU': 1,  # set to None for cpu training\n",
    "          'LR': 0.001,\n",
    "          'PRECISION': 32,\n",
    "          'CLASSES': 2,\n",
    "          'SEED': 42,\n",
    "          'PROJECT': 'Heads',\n",
    "          'EXPERIMENT': 'heads',\n",
    "          'MAXEPOCHS': 500,\n",
    "          'BACKBONE': 'resnet34',\n",
    "          'FPN': 'False',\n",
    "          'ANCHOR_SIZE': '((32, 64, 128, 256, 512),)',\n",
    "          'ASPECT_RATIOS': '((0.5, 1.0, 2.0),)',\n",
    "          'MIN_SIZE': 1024,\n",
    "          'MAX_SIZE': 10000,\n",
    "          'IMG_MEAN': '[0.485, 0.456, 0.406]',\n",
    "          'IMG_STD': '[0.229, 0.224, 0.225]',\n",
    "          'IOU_THRESHOLD': 0.5\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0f8bab4-464b-480b-883e-3e8e5e267ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input files\n",
    "inputs = get_filenames_of_path(pathlib.Path(params['INPUT_DIR']))\n",
    "inputs.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00c30ad2-9fd9-4090-b300-78a36fe83ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations\n",
    "transforms = ComposeSingle([\n",
    "    FunctionWrapperSingle(np.moveaxis, source=-1, destination=0),\n",
    "    FunctionWrapperSingle(normalize_01)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8277d891-0926-4005-938f-a8caf9646ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(params['MODEL_DIR'])\n",
    "model_state_dict = checkpoint['hyper_parameters']['model'].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72a78b32-2283-45ad-83c2-11337e4c2505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model init\n",
    "model = get_fasterRCNN_resnet(num_classes=int(parameters['CLASSES']),\n",
    "                              backbone_name=parameters['BACKBONE'],\n",
    "                              anchor_size=ast.literal_eval(parameters['ANCHOR_SIZE']),\n",
    "                              aspect_ratios=ast.literal_eval(parameters['ASPECT_RATIOS']),\n",
    "                              fpn=ast.literal_eval(parameters['FPN']),\n",
    "                              min_size=int(parameters['MIN_SIZE']),\n",
    "                              max_size=int(parameters['MAX_SIZE'])\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d659f25-0d7d-4a97-be15-5795e24f2e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load weights\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e59d69e-b9ca-45b3-8320-2a7d4a0e4c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1272, 1024)\n",
      "(1024, 1080)\n",
      "processing image: 0\n",
      "processing image: 0 DONE\n",
      "processing image: 1\n",
      "processing image: 1 DONE\n",
      "processing image: 2\n",
      "processing image: 2 DONE\n",
      "processing image: 3\n",
      "processing image: 3 DONE\n",
      "processing image: 4\n",
      "processing image: 4 DONE\n",
      "processing image: 5\n",
      "processing image: 5 DONE\n",
      "processing image: 6\n",
      "processing image: 6 DONE\n",
      "processing image: 7\n",
      "processing image: 7 DONE\n",
      "processing image: 8\n",
      "processing image: 8 DONE\n",
      "processing image: 9\n",
      "processing image: 9 DONE\n",
      "processing image: 10\n",
      "processing image: 10 DONE\n",
      "processing image: 11\n",
      "processing image: 11 DONE\n",
      "processing image: 12\n",
      "processing image: 12 DONE\n",
      "processing image: 13\n",
      "processing image: 13 DONE\n",
      "processing image: 14\n",
      "processing image: 14 DONE\n",
      "processing image: 15\n",
      "processing image: 15 DONE\n",
      "processing image: 16\n",
      "processing image: 16 DONE\n",
      "processing image: 17\n",
      "processing image: 17 DONE\n",
      "processing image: 18\n",
      "processing image: 18 DONE\n",
      "processing image: 19\n",
      "processing image: 19 DONE\n",
      "processing image: 20\n",
      "processing image: 20 DONE\n",
      "processing image: 21\n",
      "processing image: 21 DONE\n",
      "processing image: 22\n",
      "processing image: 22 DONE\n",
      "processing image: 23\n",
      "processing image: 23 DONE\n",
      "processing image: 24\n",
      "processing image: 24 DONE\n",
      "processing image: 25\n",
      "processing image: 25 DONE\n",
      "processing image: 26\n",
      "processing image: 26 DONE\n",
      "processing image: 27\n",
      "processing image: 27 DONE\n",
      "processing image: 28\n",
      "processing image: 28 DONE\n",
      "processing image: 29\n",
      "processing image: 29 DONE\n",
      "processing image: 30\n",
      "processing image: 30 DONE\n",
      "processing image: 31\n",
      "processing image: 31 DONE\n",
      "processing image: 32\n",
      "processing image: 32 DONE\n",
      "processing image: 33\n",
      "processing image: 33 DONE\n",
      "processing image: 34\n",
      "processing image: 34 DONE\n",
      "processing image: 35\n",
      "processing image: 35 DONE\n",
      "processing image: 36\n",
      "processing image: 36 DONE\n",
      "processing image: 37\n",
      "processing image: 37 DONE\n",
      "processing image: 38\n",
      "processing image: 38 DONE\n",
      "processing image: 39\n",
      "processing image: 39 DONE\n",
      "processing image: 40\n",
      "processing image: 40 DONE\n",
      "processing image: 41\n",
      "processing image: 41 DONE\n",
      "processing image: 42\n",
      "processing image: 42 DONE\n",
      "processing image: 43\n",
      "processing image: 43 DONE\n",
      "processing image: 44\n",
      "processing image: 44 DONE\n",
      "processing image: 45\n",
      "processing image: 45 DONE\n",
      "processing image: 46\n",
      "processing image: 46 DONE\n",
      "processing image: 47\n",
      "processing image: 47 DONE\n",
      "saved file for image: Input/000.jpg\n"
     ]
    }
   ],
   "source": [
    "# inference (cpu)\n",
    "model.eval()\n",
    "for filename in inputs : \n",
    "    x = imread(filename)\n",
    "    x=np.array(x)\n",
    "    dic = balayage_inference_single(x,(1024,1024), 512)\n",
    "    preds = inference_on_balayage(dic,model,transforms)    \n",
    "    save_json(preds, path=pathlib.Path('Output/'+ os.path.basename(filename)).with_suffix('.json'))\n",
    "    print('saved file for image: '+str(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04a2e424-4308-4586-a2ca-d0349cffc594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction files\n",
    "predictions = get_filenames_of_path(pathlib.Path('Inf√©rence/Output'))\n",
    "predictions.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66697a0b-7989-4e44-8056-b62b70a49bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prediction dataset\n",
    "iou_threshold = 0.25\n",
    "score_threshold = 0.6\n",
    "\n",
    "transforms_prediction = ComposeDouble([\n",
    "    FunctionWrapperDouble(np.moveaxis, source=-1, destination=0),\n",
    "    FunctionWrapperDouble(normalize_01),\n",
    "    FunctionWrapperDouble(apply_nms, input=False, target=True, iou_threshold=iou_threshold),\n",
    "    FunctionWrapperDouble(apply_score_threshold, input=False, target=True, score_threshold=score_threshold)\n",
    "])\n",
    "\n",
    "dataset_prediction = ObjectDetectionDataSet(inputs=inputs,\n",
    "                                            targets=predictions,\n",
    "                                            transform=transforms_prediction,\n",
    "                                            use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccb62e53-0e06-4bb5-86a3-8c23d1556ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping\n",
    "color_mapping = {\n",
    "    1: 'red',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff50e474-202d-48ac-b5d0-13d69c3b7de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize predictions\n",
    "datasetviewer_prediction = DatasetViewer(dataset_prediction, color_mapping)\n",
    "datasetviewer_prediction.napari()\n",
    "# add text properties gui\n",
    "datasetviewer_prediction.gui_text_properties(datasetviewer_prediction.shape_layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
