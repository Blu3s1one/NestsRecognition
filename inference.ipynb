{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb32a80-0e1b-4ddf-b2ff-1fe20611345b",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e496b5c5-b8f1-40b3-821c-ed62af8f54ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation et modification des limites de taille d'images ouvrables avec opencv et PIL\n",
    "import ast\n",
    "import os\n",
    "os.environ[\"OPENCV_IO_MAX_IMAGE_PIXELS\"] = str(pow(2,40))\n",
    "import pathlib\n",
    "\n",
    "import neptune\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "\n",
    "from scripts.datasets import ObjectDetectionDatasetSingle, ObjectDetectionDataSet\n",
    "from scripts.faster_RCNN import get_fasterRCNN_resnet\n",
    "from scripts.transformations import ComposeDouble\n",
    "from scripts.transformations import ComposeSingle\n",
    "from scripts.transformations import FunctionWrapperDouble\n",
    "from scripts.transformations import FunctionWrapperSingle\n",
    "from scripts.transformations import apply_nms, apply_score_threshold\n",
    "from scripts.transformations import normalize_01\n",
    "from scripts.utils import get_filenames_of_path, collate_single, save_json\n",
    "from scripts.visual import DatasetViewer\n",
    "from scripts.visual import DatasetViewerSingle\n",
    "from scripts.balayage import *\n",
    "from skimage.io import imread\n",
    "import PIL\n",
    "PIL.Image.MAX_IMAGE_PIXELS = 500000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88f582ef-cf8b-4ddd-bfce-e986bfa6a87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres à modifier en fonction du modèle\n",
    "params = {'INPUT_DIR': 'Inférence/Input',  # files to predict\n",
    "          'PREDICTIONS_PATH': 'Inférence/Output',  # where to save the predictions\n",
    "          'MODEL_DIR': 'Inférence/model/model.ckpt',  # load model from checkpoint\n",
    "          }\n",
    "parameters = {'BACKBONE': 'resnet34', # modèle choisir idéalement entre resnet18, resnet34, resnet50\n",
    "          'FPN': 'False',\n",
    "          'ANCHOR_SIZE': '((32, 64, 128, 256, 512),)',\n",
    "          'ASPECT_RATIOS': '((0.5, 1.0, 2.0),)',\n",
    "          'MIN_SIZE': 1024,\n",
    "          'MAX_SIZE': 1024,\n",
    "          'IMG_MEAN': '[0.485, 0.456, 0.406]',\n",
    "          'IMG_STD': '[0.229, 0.224, 0.225]',\n",
    "          'IOU_THRESHOLD': 0.5\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0f8bab4-464b-480b-883e-3e8e5e267ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération des images\n",
    "inputs = get_filenames_of_path(pathlib.Path(params['INPUT_DIR']))\n",
    "inputs.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00c30ad2-9fd9-4090-b300-78a36fe83ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations\n",
    "transforms = ComposeSingle([\n",
    "    FunctionWrapperSingle(np.moveaxis, source=-1, destination=0),\n",
    "    FunctionWrapperSingle(normalize_01)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8277d891-0926-4005-938f-a8caf9646ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération du modèle\n",
    "checkpoint = torch.load(params['MODEL_DIR'])\n",
    "model_state_dict = checkpoint['hyper_parameters']['model'].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72a78b32-2283-45ad-83c2-11337e4c2505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialisation du modèle\n",
    "model = get_fasterRCNN_resnet(num_classes=int(parameters['CLASSES']),\n",
    "                              backbone_name=parameters['BACKBONE'],\n",
    "                              anchor_size=ast.literal_eval(parameters['ANCHOR_SIZE']),\n",
    "                              aspect_ratios=ast.literal_eval(parameters['ASPECT_RATIOS']),\n",
    "                              fpn=ast.literal_eval(parameters['FPN']),\n",
    "                              min_size=int(parameters['MIN_SIZE']),\n",
    "                              max_size=int(parameters['MAX_SIZE'])\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d659f25-0d7d-4a97-be15-5795e24f2e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# affectation des poids\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e17c12",
   "metadata": {},
   "source": [
    "### Inférence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e59d69e-b9ca-45b3-8320-2a7d4a0e4c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference (cpu)\n",
    "model.eval()\n",
    "for filename in inputs : \n",
    "    x = imread(filename)\n",
    "    x=np.array(x)\n",
    "    dic = balayage_inference_single(x,(1024,1024), 512)\n",
    "    preds = inference_on_balayage(dic,model,transforms)    \n",
    "    save_json(preds, path=pathlib.Path('Inférence/Output/'+ os.path.basename(filename)).with_suffix('.json'))\n",
    "    print('saved file for image: '+str(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed940fc8",
   "metadata": {},
   "source": [
    "### Affichage des résulats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04a2e424-4308-4586-a2ca-d0349cffc594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction files\n",
    "predictions = get_filenames_of_path(pathlib.Path('Inférence/Output'))\n",
    "predictions.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66697a0b-7989-4e44-8056-b62b70a49bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prediction dataset\n",
    "iou_threshold = 0.25 #IntersectionOverUnion\n",
    "score_threshold = 0.95 \n",
    "\n",
    "transforms_prediction = ComposeDouble([\n",
    "    FunctionWrapperDouble(np.moveaxis, source=-1, destination=0),\n",
    "    FunctionWrapperDouble(normalize_01),\n",
    "    FunctionWrapperDouble(apply_nms, input=False, target=True, iou_threshold=iou_threshold),\n",
    "    FunctionWrapperDouble(apply_score_threshold, input=False, target=True, score_threshold=score_threshold)\n",
    "])\n",
    "\n",
    "dataset_prediction = ObjectDetectionDataSet(inputs=inputs,\n",
    "                                            targets=predictions,\n",
    "                                            transform=transforms_prediction,\n",
    "                                            use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccb62e53-0e06-4bb5-86a3-8c23d1556ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping\n",
    "color_mapping = {\n",
    "    1: 'red',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff50e474-202d-48ac-b5d0-13d69c3b7de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize predictions\n",
    "datasetviewer_prediction = DatasetViewer(dataset_prediction, color_mapping)\n",
    "datasetviewer_prediction.napari()\n",
    "# add text properties gui\n",
    "datasetviewer_prediction.gui_text_properties(datasetviewer_prediction.shape_layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
